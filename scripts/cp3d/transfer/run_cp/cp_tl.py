"""
A python wrapper around ChemProp that trains separate models using features
generated by different CP3D models.
"""

import os
import json
import argparse

from nff.utils import (bash_command, parse_args, fprint,
                       CHEMPROP_METRICS)


def get_cp_cmd(script,
               config_path,
               data_path,
               dataset_type):
    """
    Get the string for a ChemProp command.
    Args:
      script (str): the path to the chemprop script you're running
      config_path (str): path to the config file for the job
      data_path (str): path to the dataset being used
      dataset_type (str): type of problem you're doing (e.g. regression,
        classification, multiclass)
    Returns:
      cmd (str): the chemprop command
    """

    cmd = (f"python {script} --config_path {config_path} "
           f" --data_path {data_path} "
           f" --dataset_type {dataset_type}")
    return cmd


def hyperopt(cp_folder,
             hyp_folder,
             rerun):
    """
    Run hyperparameter optimization with ChemProp.
    Args:
      cp_folder (str): path to the chemprop folder on your computer
      hyp_folder (str): where you want to store your hyperparameter
        optimization models
      rerun (bool): whether to rerun hyperparameter optimization if
        `hyp_folder` already exists and has the completion file
        `best_params.json`.
    Returns:
      best_params (dict): best parameters from hyperparameter 
        optimization
    """

    # path to `best_params.json` file
    param_file = os.path.join(hyp_folder, "best_params.json")
    params_exist = os.path.isfile(param_file)

    # If it exists and you don't want to re-run, then load it
    if params_exist and (not rerun):

        fprint(f"Loading hyperparameter results from {param_file}\n")

        with open(param_file, "r") as f:
            best_params = json.load(f)
        return best_params

    # otherwise run the script and read in the results

    hyp_script = os.path.join(cp_folder, "hyperparameter_optimization.py")
    config_path = os.path.join(hyp_folder, "config.json")

    with open(config_path, "r") as f:
        config = json.load(f)

    data_path = config["data_path"]
    dataset_type = config["dataset_type"]
    cmd = get_cp_cmd(hyp_script,
                     config_path,
                     data_path,
                     dataset_type)
    cmd += f" --config_save_path {param_file}"

    fprint(f"Running hyperparameter optimization in folder {hyp_folder}\n")
    p = bash_command(cmd)
    p.wait()

    with open(param_file, "r") as f:
        best_params = json.load(f)

    return best_params


def train(cp_folder,
          train_folder):
    """
    Train a chemprop model.
    Args:
      cp_folder (str): path to the chemprop folder on your computer
      train_folder (str): where you want to store your trained models
    Returns:
      None
    """

    train_script = os.path.join(cp_folder, "train.py")
    config_path = os.path.join(train_folder, "config.json")

    with open(config_path, "r") as f:
        config = json.load(f)

    data_path = config["data_path"]
    dataset_type = config["dataset_type"]
    cmd = get_cp_cmd(train_script,
                     config_path,
                     data_path,
                     dataset_type)

    p = bash_command(cmd)
    p.wait()


def modify_config(base_config_path,
                  metric,
                  train_feat_path,
                  val_feat_path,
                  test_feat_path,
                  train_folder,
                  features_only,
                  hyp_params,
                  no_features):
    """
    Modify a chemprop config file with new parameters.
    Args:
      base_config_path (str): where your basic job config file
        is, with parameters that may or may not be changed depending
        on the given run
      metric (str): what metric you want to optimize in this run
      train_feat_path (str): where the features of your training set are
      val_feat_path (str): where the features of your validation set are
      test_feat_path (str): where the features of your test set are
      train_folder (str): where you want to store your trained models
      features_only (bool): whether to just train with the features and no
        MPNN
      hyp_params (dict): any hyperparameters that may have been optimized
      no_features (bool): Don't use external features when training model.
    Returns:
      None
    """

    with open(base_config_path, "r") as f:
        config = json.load(f)

    dic = {"metric": metric,
           "features_path": [train_feat_path],
           "separate_val_features_path": [val_feat_path],
           "separate_test_features_path": [test_feat_path],
           "save_dir": train_folder,
           "features_only": features_only,
           **hyp_params}

    config.update({key: val for key, val in
                   dic.items() if val is not None})

    if no_features:
        for key in list(config.keys()):
            if "features_path" in key:
                config.pop(key)

    new_config_path = os.path.join(train_folder, "config.json")
    if not os.path.isdir(train_folder):
        os.makedirs(train_folder)

    with open(new_config_path, "w") as f:
        json.dump(config, f, indent=4, sort_keys=True)


def modify_hyp_config(hyp_config_path,
                      metric,
                      hyp_feat_path,
                      hyp_folder,
                      features_only,
                      no_features):
    """
    Modfiy a hyperparameter optimization config file with new parameters.
    Args:
      hyp_config_path (str): where your basic hyperopt job config file
        is, with parameters that may or may not be changed depending
        on the given run
      metric (str): what metric you want to optimize in this run
      hyp_feat_path (str): path to all the features of the species that are
        part of the hyperparameter optimization (train and val from the
        real dataset).
      hyp_folder (str): where you want to store your trained models
      features_only (bool): whether to just train with the features and no
        MPNN
      no_features (bool): Don't use external features when training model.
    Returns:
      None
    """

    with open(hyp_config_path, "r") as f:
        config = json.load(f)

    dic = {"metric": metric,
           "features_path": [hyp_feat_path],
           "save_dir": hyp_folder,
           "features_only": features_only}

    config.update({key: val for key, val in
                   dic.items() if val is not None})

    if no_features:
        for key in list(config.keys()):
            if "features_path" in key:
                config.pop(key)

    new_config_path = os.path.join(hyp_folder, "config.json")
    if not os.path.isdir(hyp_folder):
        os.makedirs(hyp_folder)

    with open(new_config_path, "w") as f:
        json.dump(config, f, indent=4, sort_keys=True)


def main(base_config_path,
         hyp_config_path,
         train_folder,
         metric,
         train_feat_path,
         val_feat_path,
         test_feat_path,
         cp_folder,
         features_only,
         use_hyperopt,
         rerun_hyperopt,
         no_features,
         **kwargs):
    """
    Load pre-set features to train a ChemProp model.
    Args:
      base_config_path (str): where your basic job config file
        is, with parameters that may or may not be changed depending
        on the given run
      hyp_config_path (str): where your basic hyperopt job config file
        is, with parameters that may or may not be changed depending
        on the given run
      train_folder (str): where you want to store your trained models
      metric (str): what metric you want to optimize in this run
      train_feat_path (str): where the features of your training set are
      val_feat_path (str): where the features of your validation set are
      test_feat_path (str): where the features of your test set are
      cp_folder (str): path to the chemprop folder on your computer
      features_only (bool): whether to just train with the features and no
        MPNN
      use_hyperopt (bool): do a hyperparameter optimization before training
        the model
      rerun_hyperopt (bool): whether to rerun hyperparameter optimization if
        `hyp_folder` already exists and has the completion file
        `best_params.json`.
      no_features (bool): Don't use external features when training model.
    Returns:
      None
    """

    # if doing a hyperparameter optimization, run the optimization and get
    # the best parameters

    if use_hyperopt:

        hyp_feat_path = train_feat_path.replace("train", "hyperopt")
        hyp_folder = train_folder + "_hyp"

        modify_hyp_config(hyp_config_path=hyp_config_path,
                          metric=metric,
                          hyp_feat_path=hyp_feat_path,
                          hyp_folder=hyp_folder,
                          features_only=features_only,
                          no_features=no_features)

        hyp_params = hyperopt(cp_folder=cp_folder,
                              hyp_folder=hyp_folder,
                              rerun=rerun_hyperopt)

    else:
        hyp_params = {}

    # modify the base config with the parameters specific to this
    # run -- optimized hyperparameters, train/val/test features paths,
    # metric, etc.

    modify_config(base_config_path=base_config_path,
                  metric=metric,
                  train_feat_path=train_feat_path,
                  val_feat_path=val_feat_path,
                  test_feat_path=test_feat_path,
                  train_folder=train_folder,
                  features_only=features_only,
                  hyp_params=hyp_params,
                  no_features=no_features)

    # train the model

    train(cp_folder=cp_folder,
          train_folder=train_folder)


if __name__ == "__main__":
    parser = argparse.ArgumentParser()
    parser.add_argument("--base_config_path", type=str,
                        help=("Path to the reference config file "
                              "used to train a ChemProp model. "
                              "This file will be modified with "
                              "the arguments specified below "
                              "(metric and features paths)."
                              "If they are not specified then "
                              "the config file will not be "
                              "modified."))
    parser.add_argument("--hyp_config_path", type=str, default=None,
                        help=("Same as `base_config_path`, but "
                              "for the hyperparameter optimization "
                              "stage"))
    parser.add_argument("--use_hyperopt", action='store_true',
                        help=("Do hyperparameter optimization before "
                              "training "))
    parser.add_argument("--rerun_hyperopt", action='store_true',
                        help=("Rerun hyperparameter optimization even if "
                              "it has been done already. "))

    parser.add_argument("--metric", type=str,
                        choices=CHEMPROP_METRICS,
                        help=("Metric for which to evaluate "
                              "the model performance"),
                        default=None)
    parser.add_argument("--train_feat_path", type=str,
                        help=("Path to features file for training set"),
                        default=None)
    parser.add_argument("--val_feat_path", type=str,
                        help=("Path to features file for validation set"),
                        default=None)
    parser.add_argument("--test_feat_path", type=str,
                        help=("Path to features file for test set"),
                        default=None)
    parser.add_argument("--train_folder", type=str,
                        help=("Folder in which you will store the "
                              "ChemProp model."),
                        default=None)
    parser.add_argument("--features_only", action='store_true',
                        help=("Train model with only the stored features"))
    parser.add_argument("--cp_folder", type=str,
                        help=("Path to ChemProp folder."))
    parser.add_argument("--no_features", action="store_true",
                        help=("Don't use external features when training "
                              "model."))
    parser.add_argument('--this_config_file', type=str,
                        help=("Path to JSON file with arguments "
                              "for this script. If given, any "
                              "arguments in the file override the "
                              "command line arguments."))

    args = parse_args(parser, config_flag="this_config_file")
    main(**args.__dict__)
