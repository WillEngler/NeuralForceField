{
 "cells": [
  {
   "cell_type": "code",
   "execution_count": 1,
   "metadata": {},
   "outputs": [],
   "source": [
    "%load_ext autoreload\n",
    "%autoreload 2"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "# Non-adiabatic dynamics \n",
    "This tutorial shows how to run non-adiabatic dynamics with a trained model using the [Zhu-Nakamura surface hopping method](https://pubs.rsc.org/en/content/articlelanding/2014/cp/c4cp03498h).\n",
    "\n",
    "First let's import dependencies:\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 2,
   "metadata": {},
   "outputs": [
    {
     "data": {
      "application/vnd.jupyter.widget-view+json": {
       "model_id": "84cc992fcade402f8c64094e684a0fd0",
       "version_major": 2,
       "version_minor": 0
      },
      "text/plain": []
     },
     "metadata": {},
     "output_type": "display_data"
    }
   ],
   "source": [
    "import sys\n",
    "import os\n",
    "\n",
    "# so that NFF is in your path\n",
    "sys.path.insert(0, \"..\")\n",
    "\n",
    "import json\n",
    "from nff.md.zhu_nakamura.dynamics import CombinedZhuNakamura\n",
    "from ase.io.trajectory import Trajectory\n",
    "import nglview as nv\n",
    "\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": []
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "Now we'll find a trained model. The trained azobenzene models can be found in `NeuralForceField/models/azo_derivatives`. The sub-folders are for diabatic and adiabatic models, trained either with the full set of geometries, or with 40 species held out. There are also three models trained with different splits and different initialization from random seeds:"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 3,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "['all_diabatic', 'holdout_diabatic', 'holdout_adiabatic']\n",
      "['seed_0', 'seed_2', 'seed_1']\n"
     ]
    }
   ],
   "source": [
    "print(os.listdir('../models/azo_derivatives'))\n",
    "print(os.listdir('../models/azo_derivatives/all_diabatic'))"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "We'll use the diabatic model trained on all species, with seed 0: `../models/azo_derivatives/all_diabatic/seed_0`."
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "## ZN\n",
    "\n",
    "The script for Tully's surface hopping is `NeuralForceField/nff/md/zhu_nakamura/run_zn`. If you run the script and supply the path of a JSON parameter file, it will do the rest for you. Here we'll go through some parameters to give, and show a little of what goes on behind the scenes.\n",
    "\n",
    "We'll have to define `ground_params`, `zhu_params`, `combined_params`, and `all_params`. The first is for parameters in the ground state MD simulation, the second for ZN surface hopping, and the third is for batching. The fourth is for some remaining parameters, which we'll explain below.\n",
    "\n",
    "Let's define `ground_params`:"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 4,
   "metadata": {},
   "outputs": [],
   "source": [
    "ground_params = {'ttime': 50,\n",
    "                 'logfile': 'ground.log',\n",
    "                 'max_time': 200,\n",
    "                 'savefile': 'ground.trj',\n",
    "                 'timestep': 0.5,\n",
    "                 'equil_time': 100,\n",
    "                 'thermostat': 'nosehoover',\n",
    "                 'loginterval': 10,\n",
    "                 'temperature': 300}"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "Now let's do `zhu_params` and `batched_params`:"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 5,
   "metadata": {},
   "outputs": [],
   "source": [
    "zhu_params = {'log_file': 'trj.log',\n",
    " 'max_time': 200,\n",
    " 'out_file': 'trj.csv',\n",
    " 'timestep': 0.5,\n",
    " 'num_states': 2,\n",
    " 'max_gap_hop': 0.021673306772908366,\n",
    " 'save_period': 5,\n",
    " 'initial_surf': 1,\n",
    " 'initial_time': 0.0}\n",
    "\n",
    "batched_params = {'cutoff': 5.0,\n",
    " 'device': 1,\n",
    " 'num_trj': 10,\n",
    " 'batch_size': 5,\n",
    " 'nbr_update_period': 10}"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "Lastly we'll define `all_params`, which has the starting coordinates and the model path:"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 6,
   "metadata": {},
   "outputs": [],
   "source": [
    "with open('data/azo_coords.json', 'r') as f:\n",
    "    coords = json.load(f)\n",
    "\n",
    "all_params = {\"coords\": coords, # starting geometry of the molecule\n",
    "              'model_path': '../models/azo_derivatives/all_diabatic/seed_0',\n",
    "              \"cutoff\": 5.0,\n",
    "              \"zhu_params\": zhu_params,\n",
    "              \"ground_params\": ground_params,\n",
    "              \"batched_params\": batched_params\n",
    "             }"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "When we run the script from the command line, it parses these three dictionaries from a file and makes an instance of `CombinedZhuNakamura`, like this:"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 7,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "loading models\n",
      "running ground state + Zhu-Nakamura dynamics\n"
     ]
    },
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "/home/saxelrod/miniconda3/envs/nff/lib/python3.7/site-packages/ase/md/md.py:48: FutureWarning: Specify the temperature in K using the 'temperature_K' argument\n",
      "  warnings.warn(FutureWarning(w))\n"
     ]
    }
   ],
   "source": [
    "from nff.md.zhu_nakamura.run_zn import coords_to_xyz, make_dataset, make_trj\n",
    "\n",
    "coords = all_params[\"coords\"]\n",
    "nxyz = [coords_to_xyz(coords)]\n",
    "\n",
    "\n",
    "print('loading models')\n",
    "\n",
    "dataset = make_dataset(nxyz=nxyz, all_params=all_params)\n",
    "\n",
    "print('running ground state + Zhu-Nakamura dynamics')\n",
    "\n",
    "zn = make_trj(all_params=all_params,\n",
    "              dataset=dataset)\n",
    "\n"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "Now we can run it!"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 8,
   "metadata": {},
   "outputs": [
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "../nff/nn/modules/diabat.py:140: UserWarning: torch.symeig is deprecated in favor of torch.linalg.eigh and will be removed in a future PyTorch release.\n",
      "The default behavior has changed from using the upper triangular portion of the matrix by default to using the lower triangular portion.\n",
      "L, _ = torch.symeig(A, upper=upper)\n",
      "should be replaced with\n",
      "L = torch.linalg.eigvalsh(A, UPLO='U' if upper else 'L')\n",
      "and\n",
      "L, V = torch.symeig(A, eigenvectors=True)\n",
      "should be replaced with\n",
      "L, V = torch.linalg.eigh(A, UPLO='U' if upper else 'L') (Triggered internally at  /opt/conda/conda-bld/pytorch_1623448224956/work/aten/src/ATen/native/BatchLinearAlgebra.cpp:2500.)\n",
      "  ad_energies, u = torch.symeig(d_mat, True)\n"
     ]
    },
    {
     "ename": "RuntimeError",
     "evalue": "CUDA out of memory. Tried to allocate 20.00 MiB (GPU 0; 10.76 GiB total capacity; 433.06 MiB already allocated; 8.44 MiB free; 508.00 MiB reserved in total by PyTorch)",
     "output_type": "error",
     "traceback": [
      "\u001b[0;31m---------------------------------------------------------------------------\u001b[0m",
      "\u001b[0;31mRuntimeError\u001b[0m                              Traceback (most recent call last)",
      "\u001b[0;32m<ipython-input-8-d4fefc506dbb>\u001b[0m in \u001b[0;36m<module>\u001b[0;34m\u001b[0m\n\u001b[0;32m----> 1\u001b[0;31m \u001b[0mzn\u001b[0m\u001b[0;34m.\u001b[0m\u001b[0mrun\u001b[0m\u001b[0;34m(\u001b[0m\u001b[0;34m)\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[0m",
      "\u001b[0;32m~/Repo/projects/master/NeuralForceField/nff/md/zhu_nakamura/dynamics.py\u001b[0m in \u001b[0;36mrun\u001b[0;34m(self)\u001b[0m\n\u001b[1;32m   1387\u001b[0m                                         needs_angles=self.needs_angles)\n\u001b[1;32m   1388\u001b[0m \u001b[0;34m\u001b[0m\u001b[0m\n\u001b[0;32m-> 1389\u001b[0;31m         \u001b[0mbatched_zn\u001b[0m\u001b[0;34m.\u001b[0m\u001b[0mrun\u001b[0m\u001b[0;34m(\u001b[0m\u001b[0;34m)\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[0m",
      "\u001b[0;32m~/Repo/projects/master/NeuralForceField/nff/md/zhu_nakamura/dynamics.py\u001b[0m in \u001b[0;36mrun\u001b[0;34m(self)\u001b[0m\n\u001b[1;32m   1239\u001b[0m         \u001b[0;31m# initial energy and force calculation to get things started\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[1;32m   1240\u001b[0m         self.update_energies_forces(trjs=self.zhu_trjs,\n\u001b[0;32m-> 1241\u001b[0;31m                                     get_new_neighbors=True)\n\u001b[0m\u001b[1;32m   1242\u001b[0m         \u001b[0mcomplete\u001b[0m \u001b[0;34m=\u001b[0m \u001b[0;32mFalse\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[1;32m   1243\u001b[0m         \u001b[0mnum_steps\u001b[0m \u001b[0;34m=\u001b[0m \u001b[0;36m0\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n",
      "\u001b[0;32m~/Repo/projects/master/NeuralForceField/nff/md/zhu_nakamura/dynamics.py\u001b[0m in \u001b[0;36mupdate_energies_forces\u001b[0;34m(self, trjs, get_new_neighbors)\u001b[0m\n\u001b[1;32m   1046\u001b[0m         \u001b[0;32mfor\u001b[0m \u001b[0mi\u001b[0m\u001b[0;34m,\u001b[0m \u001b[0mbatch\u001b[0m \u001b[0;32min\u001b[0m \u001b[0menumerate\u001b[0m\u001b[0;34m(\u001b[0m\u001b[0mloader\u001b[0m\u001b[0;34m)\u001b[0m\u001b[0;34m:\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[1;32m   1047\u001b[0m             \u001b[0mbatch\u001b[0m \u001b[0;34m=\u001b[0m \u001b[0mbatch_to\u001b[0m\u001b[0;34m(\u001b[0m\u001b[0mbatch\u001b[0m\u001b[0;34m,\u001b[0m \u001b[0mself\u001b[0m\u001b[0;34m.\u001b[0m\u001b[0mdevice\u001b[0m\u001b[0;34m)\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[0;32m-> 1048\u001b[0;31m             \u001b[0mresults\u001b[0m \u001b[0;34m=\u001b[0m \u001b[0mself\u001b[0m\u001b[0;34m.\u001b[0m\u001b[0mmodel\u001b[0m\u001b[0;34m(\u001b[0m\u001b[0mbatch\u001b[0m\u001b[0;34m)\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[0m\u001b[1;32m   1049\u001b[0m             \u001b[0mresults\u001b[0m \u001b[0;34m=\u001b[0m \u001b[0mbatch_detach\u001b[0m\u001b[0;34m(\u001b[0m\u001b[0mresults\u001b[0m\u001b[0;34m)\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[1;32m   1050\u001b[0m \u001b[0;34m\u001b[0m\u001b[0m\n",
      "\u001b[0;32m~/miniconda3/envs/nff/lib/python3.7/site-packages/torch/nn/modules/module.py\u001b[0m in \u001b[0;36m_call_impl\u001b[0;34m(self, *input, **kwargs)\u001b[0m\n\u001b[1;32m   1049\u001b[0m         if not (self._backward_hooks or self._forward_hooks or self._forward_pre_hooks or _global_backward_hooks\n\u001b[1;32m   1050\u001b[0m                 or _global_forward_hooks or _global_forward_pre_hooks):\n\u001b[0;32m-> 1051\u001b[0;31m             \u001b[0;32mreturn\u001b[0m \u001b[0mforward_call\u001b[0m\u001b[0;34m(\u001b[0m\u001b[0;34m*\u001b[0m\u001b[0minput\u001b[0m\u001b[0;34m,\u001b[0m \u001b[0;34m**\u001b[0m\u001b[0mkwargs\u001b[0m\u001b[0;34m)\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[0m\u001b[1;32m   1052\u001b[0m         \u001b[0;31m# Do not call functions when jit is used\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[1;32m   1053\u001b[0m         \u001b[0mfull_backward_hooks\u001b[0m\u001b[0;34m,\u001b[0m \u001b[0mnon_full_backward_hooks\u001b[0m \u001b[0;34m=\u001b[0m \u001b[0;34m[\u001b[0m\u001b[0;34m]\u001b[0m\u001b[0;34m,\u001b[0m \u001b[0;34m[\u001b[0m\u001b[0;34m]\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n",
      "\u001b[0;32m~/Repo/projects/master/NeuralForceField/nff/nn/models/painn.py\u001b[0m in \u001b[0;36mforward\u001b[0;34m(self, batch, xyz, add_nacv, add_grad, add_gap, add_u, inference, do_nan)\u001b[0m\n\u001b[1;32m    390\u001b[0m                                         \u001b[0madd_u\u001b[0m\u001b[0;34m=\u001b[0m\u001b[0madd_u\u001b[0m\u001b[0;34m,\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[1;32m    391\u001b[0m                                         \u001b[0minference\u001b[0m\u001b[0;34m=\u001b[0m\u001b[0minference\u001b[0m\u001b[0;34m,\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[0;32m--> 392\u001b[0;31m                                         do_nan=do_nan)\n\u001b[0m\u001b[1;32m    393\u001b[0m \u001b[0;34m\u001b[0m\u001b[0m\n\u001b[1;32m    394\u001b[0m         \u001b[0;32mreturn\u001b[0m \u001b[0mresults\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n",
      "\u001b[0;32m~/miniconda3/envs/nff/lib/python3.7/site-packages/torch/nn/modules/module.py\u001b[0m in \u001b[0;36m_call_impl\u001b[0;34m(self, *input, **kwargs)\u001b[0m\n\u001b[1;32m   1049\u001b[0m         if not (self._backward_hooks or self._forward_hooks or self._forward_pre_hooks or _global_backward_hooks\n\u001b[1;32m   1050\u001b[0m                 or _global_forward_hooks or _global_forward_pre_hooks):\n\u001b[0;32m-> 1051\u001b[0;31m             \u001b[0;32mreturn\u001b[0m \u001b[0mforward_call\u001b[0m\u001b[0;34m(\u001b[0m\u001b[0;34m*\u001b[0m\u001b[0minput\u001b[0m\u001b[0;34m,\u001b[0m \u001b[0;34m**\u001b[0m\u001b[0mkwargs\u001b[0m\u001b[0;34m)\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[0m\u001b[1;32m   1052\u001b[0m         \u001b[0;31m# Do not call functions when jit is used\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[1;32m   1053\u001b[0m         \u001b[0mfull_backward_hooks\u001b[0m\u001b[0;34m,\u001b[0m \u001b[0mnon_full_backward_hooks\u001b[0m \u001b[0;34m=\u001b[0m \u001b[0;34m[\u001b[0m\u001b[0;34m]\u001b[0m\u001b[0;34m,\u001b[0m \u001b[0;34m[\u001b[0m\u001b[0;34m]\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n",
      "\u001b[0;32m~/Repo/projects/master/NeuralForceField/nff/nn/modules/diabat.py\u001b[0m in \u001b[0;36mforward\u001b[0;34m(self, batch, xyz, results, add_nacv, add_grad, add_gap, add_u, inference, do_nan)\u001b[0m\n\u001b[1;32m    489\u001b[0m                                          \u001b[0mnum_atoms\u001b[0m\u001b[0;34m=\u001b[0m\u001b[0mnum_atoms\u001b[0m\u001b[0;34m,\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[1;32m    490\u001b[0m                                          \u001b[0mu\u001b[0m\u001b[0;34m=\u001b[0m\u001b[0mu\u001b[0m\u001b[0;34m,\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[0;32m--> 491\u001b[0;31m                                          inference=inference)\n\u001b[0m\u001b[1;32m    492\u001b[0m         \u001b[0;32melif\u001b[0m \u001b[0madd_grad\u001b[0m\u001b[0;34m:\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[1;32m    493\u001b[0m             results = self.add_adiabat_grads(xyz=xyz,\n",
      "\u001b[0;32m~/Repo/projects/master/NeuralForceField/nff/nn/modules/diabat.py\u001b[0m in \u001b[0;36madd_all_grads\u001b[0;34m(self, xyz, results, num_atoms, u, inference)\u001b[0m\n\u001b[1;32m    199\u001b[0m                                                       \u001b[0mxyz\u001b[0m\u001b[0;34m=\u001b[0m\u001b[0mxyz\u001b[0m\u001b[0;34m,\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[1;32m    200\u001b[0m                                                       \u001b[0mnum_atoms\u001b[0m\u001b[0;34m=\u001b[0m\u001b[0mnum_atoms\u001b[0m\u001b[0;34m,\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[0;32m--> 201\u001b[0;31m                                                       inference=inference)\n\u001b[0m\u001b[1;32m    202\u001b[0m         split_grads = torch.split(diabat_grads,\n\u001b[1;32m    203\u001b[0m                                   num_atoms, dim=2)\n",
      "\u001b[0;32m~/Repo/projects/master/NeuralForceField/nff/nn/modules/diabat.py\u001b[0m in \u001b[0;36mget_diabat_grads\u001b[0;34m(self, results, xyz, num_atoms, inference)\u001b[0m\n\u001b[1;32m    181\u001b[0m                     grad = compute_grad(inputs=xyz,\n\u001b[1;32m    182\u001b[0m                                         \u001b[0moutput\u001b[0m\u001b[0;34m=\u001b[0m\u001b[0mresults\u001b[0m\u001b[0;34m[\u001b[0m\u001b[0mdiabat_key\u001b[0m\u001b[0;34m]\u001b[0m\u001b[0;34m,\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[0;32m--> 183\u001b[0;31m                                         allow_unused=True)\n\u001b[0m\u001b[1;32m    184\u001b[0m \u001b[0;34m\u001b[0m\u001b[0m\n\u001b[1;32m    185\u001b[0m                 \u001b[0;32mif\u001b[0m \u001b[0minference\u001b[0m\u001b[0;34m:\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n",
      "\u001b[0;32m~/Repo/projects/master/NeuralForceField/nff/utils/scatter.py\u001b[0m in \u001b[0;36mcompute_grad\u001b[0;34m(inputs, output, allow_unused)\u001b[0m\n\u001b[1;32m     23\u001b[0m                       \u001b[0mcreate_graph\u001b[0m\u001b[0;34m=\u001b[0m\u001b[0;32mTrue\u001b[0m\u001b[0;34m,\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[1;32m     24\u001b[0m                       \u001b[0mretain_graph\u001b[0m\u001b[0;34m=\u001b[0m\u001b[0;32mTrue\u001b[0m\u001b[0;34m,\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[0;32m---> 25\u001b[0;31m                       allow_unused=allow_unused)\n\u001b[0m\u001b[1;32m     26\u001b[0m \u001b[0;34m\u001b[0m\u001b[0m\n\u001b[1;32m     27\u001b[0m     \u001b[0;32mreturn\u001b[0m \u001b[0mgradspred\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n",
      "\u001b[0;32m~/miniconda3/envs/nff/lib/python3.7/site-packages/torch/autograd/__init__.py\u001b[0m in \u001b[0;36mgrad\u001b[0;34m(outputs, inputs, grad_outputs, retain_graph, create_graph, only_inputs, allow_unused)\u001b[0m\n\u001b[1;32m    226\u001b[0m     return Variable._execution_engine.run_backward(\n\u001b[1;32m    227\u001b[0m         \u001b[0moutputs\u001b[0m\u001b[0;34m,\u001b[0m \u001b[0mgrad_outputs_\u001b[0m\u001b[0;34m,\u001b[0m \u001b[0mretain_graph\u001b[0m\u001b[0;34m,\u001b[0m \u001b[0mcreate_graph\u001b[0m\u001b[0;34m,\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[0;32m--> 228\u001b[0;31m         inputs, allow_unused, accumulate_grad=False)\n\u001b[0m\u001b[1;32m    229\u001b[0m \u001b[0;34m\u001b[0m\u001b[0m\n\u001b[1;32m    230\u001b[0m \u001b[0;34m\u001b[0m\u001b[0m\n",
      "\u001b[0;31mRuntimeError\u001b[0m: CUDA out of memory. Tried to allocate 20.00 MiB (GPU 0; 10.76 GiB total capacity; 433.06 MiB already allocated; 8.44 MiB free; 508.00 MiB reserved in total by PyTorch)"
     ]
    }
   ],
   "source": [
    "zn.run()\n"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "We can view the ground-state log file:"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {
    "scrolled": true
   },
   "outputs": [],
   "source": [
    "with open('ground.log', 'r') as f:\n",
    "    ground_log = f.read()\n",
    "print(ground_log)"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "We see that all energies fluctuate, as kinetic energy is being added into the system fo the thermostat. The temperature also varies, and over enough time it will average out to 300 K. \n",
    "\n",
    "To get the actual geometries, energies, and forces, we can load the trajectory file. And we can visualize it with `nglview`:\n",
    "\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "trj = Trajectory('ground.trj')\n",
    "nv.show_asetraj(trj)"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "We can also view the excited state log:"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {
    "scrolled": true
   },
   "outputs": [],
   "source": [
    "with open('tully.log', 'r') as f:\n",
    "    tully_log = f.read()\n",
    "print(tully_log)"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "This log shows us the population in the different states at each time, the norm of the electronic coefficient vector $c$, and the maximum hopping probability of any of the trajectories. We see that all of the trajectories have returned to the ground state by 200 fs.\n",
    "\n",
    "To get the geometries, forces, etc., we can use the `NeuralTully` class method `from_pickle`:\n",
    "\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "state_dicts, trjs = NeuralTully.from_pickle('tully.pickle')"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "The output contains `state_dicts`, a set of dictionaries with information about each trajectory for each time step, and `trjs`, a set of ASE trajectories. We can visualize one of the trajectories:"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "trj = trjs[0]\n",
    "nv.show_asetraj(trj)"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "We can also take a look at `state_dicts`:"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "print(len(state_dicts))\n",
    "print(list(state_dicts[0].keys()))\n"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "We see that there is one dictionary for each saved time step (all time steps in which a hop occurs are automatically saved, even if you only asked the geoms to be saved every X steps). Each dictionary has various properties, each of shape `num_trj  x ...`, so that the first dimension divides properties by trajectory. For example:\n",
    "\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {
    "scrolled": true
   },
   "outputs": [],
   "source": [
    "state_dict = state_dicts[0]\n",
    "print('nxyz: ', state_dict['nxyz'].shape) # coordinates \n",
    "print('energy: ', state_dict['energy'].shape) # energy\n",
    "print('forces: ', state_dict['forces'].shape) # forces on each surface\n",
    "print('H_d: ', state_dict['H_d'].shape) # diabatic Hamiltonian\n",
    "print('U: ', state_dict['U'].shape) # unitary transformation from diabatic to adiabatic \n",
    "print('vel: ', state_dict['vel'].shape) # velocities\n",
    "print('c: ', state_dict['c'].shape) # c electronic wavefunction vector \n",
    "print('surfs: ', state_dict['surfs'].shape) # current surface\n",
    "\n",
    "\n",
    "print(state_dict['force_nacv']) # not computed because we did diabatic propagation\n",
    "print(state_dict['T']) # T matrix (NACV off-diagonal components of Hamiltonian)\n",
    "                       # also not computed because we did diabatic propagation"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": []
  }
 ],
 "metadata": {
  "kernelspec": {
   "display_name": "Python [conda env:nff]",
   "language": "python",
   "name": "nff"
  },
  "language_info": {
   "codemirror_mode": {
    "name": "ipython",
    "version": 3
   },
   "file_extension": ".py",
   "mimetype": "text/x-python",
   "name": "python",
   "nbconvert_exporter": "python",
   "pygments_lexer": "ipython3",
   "version": "3.7.10"
  }
 },
 "nbformat": 4,
 "nbformat_minor": 2
}
