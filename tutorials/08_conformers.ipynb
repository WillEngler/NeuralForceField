{
 "cells": [
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "# Introduction to Neural Force Field"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "This Jupyter Notebook contains an introduction to the `nff` package. Here, we will load the modules and functions from `nff` to import a dataset, create dataloaders, create a model, train it and check the test stats. We will do most of it manually to illustrate the usage of the API. However, scripts such as the one provided in the `scripts/` folder already automate most of this process."
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "After the `nff` package has been installed, we start by importing all dependencies for this tutorial."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 1,
   "metadata": {},
   "outputs": [],
   "source": [
    "%load_ext autoreload\n",
    "%autoreload 2"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": []
  },
  {
   "cell_type": "code",
   "execution_count": 2,
   "metadata": {},
   "outputs": [],
   "source": [
    "import sys\n",
    "sys.path.insert(0, \"..\")\n",
    "# sys.path.insert(0, \"/home/saxelrod/Repo/projects/covid_nff/NeuralForceField\")\n",
    "# sys.path.remove('/home/saxelrod/Repo/projects/ax_autopology/NeuralForceField')\n",
    "\n",
    "import os\n",
    "import shutil\n",
    "import numpy as np\n",
    "import matplotlib.pyplot as plt\n",
    "\n",
    "import torch\n",
    "from torch.optim import Adam\n",
    "from torch.utils.data import DataLoader\n",
    "\n",
    "from nff.data import Dataset, split_train_validation_test, collate_dicts, to_tensor\n",
    "from nff.train import Trainer, get_trainer, get_model, load_model, loss, hooks, metrics, evaluate"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": []
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "It might also be useful setting the GPU you want to use:"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 3,
   "metadata": {},
   "outputs": [],
   "source": [
    "# DEVICE = 1\n",
    "# OUTDIR = './sandbox'\n",
    "# model = load_model(OUTDIR)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 4,
   "metadata": {},
   "outputs": [],
   "source": [
    "DEVICE = 3\n",
    "OUTDIR = './sandbox'\n",
    "\n",
    "if os.path.exists(OUTDIR):\n",
    "    newpath = os.path.join(os.path.dirname(OUTDIR), 'backup')\n",
    "    if os.path.exists(newpath):\n",
    "        shutil.rmtree(newpath)\n",
    "        \n",
    "    shutil.move(OUTDIR, newpath)"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "## Loading the relevant data"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "As we usually work with the database, we can pack their information in a class `Dataset`, which is a subclass of `torch.utils.data.Dataset`. It basically wraps information on the atomic numbers, energies, forces and SMILES strings for each one of the geometries. In this example, we already have a pre-compiled `Dataset` to be used. We start by loading this file and creating three slices of the original dataset"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 5,
   "metadata": {},
   "outputs": [],
   "source": [
    "# dataset = Dataset.from_file('./data/covid.pth.tar')\n",
    "dataset = Dataset.from_file('./data/covid_mmff94.pth.tar')\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 6,
   "metadata": {},
   "outputs": [],
   "source": [
    "import pdb\n",
    "\n",
    "def separate_datasets(dataset, split_ratio):\n",
    "\n",
    "    bind_indices = torch.LongTensor([i  for i, bind in enumerate(dataset.props['bind']) if bind])\n",
    "    remaining_indices = [i for i in range(len(dataset)) if i not in bind_indices]\n",
    "\n",
    "    fail_dataset = dataset.copy()\n",
    "    for key, val in fail_dataset.props.items():\n",
    "        fail_dataset.props[key] = [val[i] for i in remaining_indices]\n",
    "    return dataset, fail_dataset, bind_indices\n",
    "\n",
    "def get_split_bind_indices(bind_indices, split_ratio):\n",
    "    num_bind = len(bind_indices)\n",
    "    bind_per_split = (split_ratio * num_bind).astype('int')\n",
    "    while True:\n",
    "        for i in range(3):\n",
    "            if sum(bind_per_split) == num_bind:\n",
    "                break\n",
    "            bind_per_split[i] += 1\n",
    "        if sum(bind_per_split) == num_bind:\n",
    "                break\n",
    "\n",
    "    bind_per_split = bind_per_split.tolist()\n",
    "    split_bind_indices = torch.split(bind_indices, bind_per_split)\n",
    "    return split_bind_indices\n",
    "\n",
    "def make_bind_datasets(split_bind_indices, dataset):\n",
    "    \n",
    "    datasets = []\n",
    "    for indices in split_bind_indices:\n",
    "        new_set = dataset.copy()\n",
    "        for key, val in dataset.props.items():\n",
    "            new_set.props[key] = to_tensor([val[i] for i in indices])\n",
    "        datasets.append(new_set)\n",
    "    return tuple(datasets)\n",
    "    \n",
    "\n",
    "def split_data(dataset, split_ratio):\n",
    "    dataset, fail_dataset, bind_indices = separate_datasets(dataset, split_ratio)\n",
    "    split_bind_indices =  get_split_bind_indices(bind_indices, split_ratio)\n",
    "    bind_datasets = make_bind_datasets(split_bind_indices, dataset)\n",
    "    \n",
    "    train, val, test = split_train_validation_test(fail_dataset, val_size=0.2, test_size=0.2)\n",
    "    split_sets = [train, val, test]\n",
    "    \n",
    "    for i in range(3):\n",
    "        split_set = split_sets[i]\n",
    "        bind_set = bind_datasets[i]\n",
    "        \n",
    "        for key, value in bind_set.props.items():\n",
    "            if type(value) is list:\n",
    "                split_set.props[key] += value\n",
    "            else:\n",
    "                split_set.props[key] = torch.cat((split_set.props[key], value))\n",
    "    \n",
    "    return train, val, test\n",
    "\n",
    "        "
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 7,
   "metadata": {},
   "outputs": [],
   "source": [
    "split_ratio = np.array([0.6, 0.2, 0.2])\n",
    "train, val, test = split_data(dataset, split_ratio)"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "The `nff` code interfaces with the `graphbuilder` module through a git submodule in the repository. `graphbuilder` provides methods to create batches of graphs. In `nff`, we interface that through a custom dataloader called `\n",
    "GraphLoader`. Here, we create one loader for each one of the slices."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 8,
   "metadata": {},
   "outputs": [],
   "source": [
    "train_loader = DataLoader(train, batch_size=10, collate_fn=collate_dicts)\n",
    "val_loader = DataLoader(val, batch_size=10, collate_fn=collate_dicts)\n",
    "test_loader = DataLoader(test, batch_size=10, collate_fn=collate_dicts)"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "Number of positive binders in train, validation, and test sets:"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 9,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "tensor(163)\n",
      "tensor(54)\n",
      "tensor(54)\n"
     ]
    }
   ],
   "source": [
    "print(torch.sum(train.props['bind']))\n",
    "print(torch.sum(val.props['bind']))\n",
    "print(torch.sum(test.props['bind']))\n",
    "\n"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "## Creating a model"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "`nff` is based on SchNet. It parameterizes interatomic interactions in molecules and materials through a series of convolution layers with continuous filters. Here, we are going to create a simple model using the hyperparameters given on `params`:"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 10,
   "metadata": {},
   "outputs": [],
   "source": [
    "\n",
    "\n",
    "n_atom_basis = 256\n",
    "mol_basis = 256\n",
    "\n",
    "mol_fp_layers = [{'name': 'linear', 'param' : { 'in_features': n_atom_basis,\n",
    "                                                              'out_features': int((n_atom_basis + mol_basis)/2)}},\n",
    "                               {'name': 'shifted_softplus', 'param': {}},\n",
    "                               {'name': 'linear', 'param' : { 'in_features': int((n_atom_basis + mol_basis)/2),\n",
    "                                                              'out_features': mol_basis}}]\n",
    "\n",
    "readoutdict = {\n",
    "                    \"bind\": [{'name': 'linear', 'param' : { 'in_features': mol_basis,\n",
    "                                                              'out_features': int(mol_basis / 2)}},\n",
    "                               {'name': 'shifted_softplus', 'param': {}},\n",
    "                               {'name': 'linear', 'param' : { 'in_features': int(mol_basis / 2),\n",
    "                                                              'out_features': 1}},\n",
    "                               {'name': 'sigmoid', 'param': {}}],\n",
    "                }\n",
    "\n",
    "params = {\n",
    "    'n_atom_basis': n_atom_basis,\n",
    "    'n_filters': 256,\n",
    "    'n_gaussians': 32,\n",
    "    'n_convolutions': 4,\n",
    "    'cutoff': 5.0,\n",
    "    'trainable_gauss': True,\n",
    "    'dropout_rate': 0.2,\n",
    "    'mol_fp_layers': mol_fp_layers,\n",
    "    'readoutdict': readoutdict\n",
    "}\n",
    "\n",
    "\n",
    "model = get_model(params=params, model_type='WeightedConformers')"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "## Creating a trainer"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "To train our model with the data provided, we have to create a loss function. The easiest way to do that is through the `build_mse_loss` builder. Its argument `rho` is a parameter that will multiply the mean square error (MSE) of the force components before summing it with the MSE of the energy."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": []
  },
  {
   "cell_type": "code",
   "execution_count": 11,
   "metadata": {},
   "outputs": [],
   "source": [
    "loss_fn = loss.build_cross_entropy_loss(loss_coef={'bind': 1.0})"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "We should also select an optimizer for our recently created model:"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 12,
   "metadata": {},
   "outputs": [],
   "source": [
    "trainable_params = filter(lambda p: p.requires_grad, model.parameters())\n",
    "optimizer = Adam(trainable_params, lr=3e-4)"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "### Metrics and hooks"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "Metrics and hooks allow the customization of the training process. Instead of tweaking directly the code or having to resort to countless flags, we can create submodules (or add-ons) to monitor the progress of the training or customize it."
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "If we want to monitor the progress of our training, say by looking at the mean absolute error (MAE) of energies and forces, we can simply create metrics to observe them:"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 13,
   "metadata": {},
   "outputs": [],
   "source": [
    "train_metrics = [\n",
    "    metrics.TruePositives('bind'),\n",
    "    metrics.TrueNegatives('bind'),\n",
    "    metrics.FalsePositives('bind'),\n",
    "    metrics.FalseNegatives('bind'),\n",
    "\n",
    "]"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "Furthermore, if we want to customize how our training procedure is done, we can use hooks which can interrupt or change the train automatically.\n",
    "\n",
    "In our case, we are adding hooks to:\n",
    "* Stop the training procedure after 100 epochs;\n",
    "* Log the training on a machine-readable CSV file under the directory `./sandbox`;\n",
    "* Print the progress on the screen with custom formatting; and\n",
    "* Setup a scheduler for the learning rate."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 14,
   "metadata": {},
   "outputs": [],
   "source": [
    "train_hooks = [\n",
    "    hooks.MaxEpochHook(100),\n",
    "    hooks.CSVHook(\n",
    "        OUTDIR,\n",
    "        metrics=train_metrics,\n",
    "    ),\n",
    "    hooks.PrintingHook(\n",
    "        OUTDIR,\n",
    "        metrics=train_metrics,\n",
    "        separator = ' | ',\n",
    "        time_strf='%M:%S'\n",
    "    ),\n",
    "    hooks.ReduceLROnPlateauHook(\n",
    "        optimizer=optimizer,\n",
    "        patience=30,\n",
    "        factor=0.5,\n",
    "        min_lr=1e-7,\n",
    "        window_length=1,\n",
    "        stop_after_min=True\n",
    "    )\n",
    "]"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "### Trainer wrapper"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "A `Trainer` in the `nff` package is a wrapper to train a model. It automatically creates checkpoints, as well as trains and validates a given model. It also allow further training by loading checkpoints from existing paths, making the training procedure more flexible. Its functionalities can be extended by the hooks we created above. To create a trainer, we have to execute the following command:"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 15,
   "metadata": {},
   "outputs": [],
   "source": [
    "T = Trainer(\n",
    "    model_path=OUTDIR,\n",
    "    model=model,\n",
    "    loss_fn=loss_fn,\n",
    "    optimizer=optimizer,\n",
    "    train_loader=train_loader,\n",
    "    validation_loader=val_loader,\n",
    "    checkpoint_interval=1,\n",
    "    hooks=train_hooks\n",
    ")"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "Now we can finally train the model using the method `train` from the `Trainer`:"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": []
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {
    "scrolled": false
   },
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      " Time | Epoch | Learning rate | Train loss | Validation loss | TruePositive_bind | TrueNegative_bind | FalsePositive_bind | FalseNegative_bind | GPU Memory (MB)\n"
     ]
    },
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "../nff/train/metrics.py:45: RuntimeWarning: invalid value encountered in double_scalars\n",
      "  return self.loss / self.n_entries\n"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "51:08 |     1 |     3.000e-04 |     1.4861 |          1.3004 |               nan |            0.9573 |                nan |             0.0427 |               0\n",
      "51:32 |     2 |     3.000e-04 |     1.4827 |          1.3051 |               nan |            0.9573 |                nan |             0.0427 |               0\n",
      "51:56 |     3 |     3.000e-04 |     1.4827 |          1.3051 |               nan |            0.9573 |                nan |             0.0427 |               0\n"
     ]
    }
   ],
   "source": [
    "T.train(device=DEVICE, n_epochs=100)\n"
   ]
  }
 ],
 "metadata": {
  "kernelspec": {
   "display_name": "Python [conda env:neuralff]",
   "language": "python",
   "name": "neuralff"
  },
  "language_info": {
   "codemirror_mode": {
    "name": "ipython",
    "version": 3
   },
   "file_extension": ".py",
   "mimetype": "text/x-python",
   "name": "python",
   "nbconvert_exporter": "python",
   "pygments_lexer": "ipython3",
   "version": "3.7.5"
  }
 },
 "nbformat": 4,
 "nbformat_minor": 2
}
