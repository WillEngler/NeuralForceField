{
 "cells": [
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "# Introduction to Neural Force Field"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "This Jupyter Notebook contains an introduction to the `nff` package. Here, we will load the modules and functions from `nff` to import a dataset, create dataloaders, create a model, train it and check the test stats. We will do most of it manually to illustrate the usage of the API. However, scripts such as the one provided in the `scripts/` folder already automate most of this process."
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "After the `nff` package has been installed, we start by importing all dependencies for this tutorial."
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "**Multi-task**: Change `two_states` to False if you just want one state. Change `one_mol` to True if you just want one molecule."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 1,
   "metadata": {},
   "outputs": [],
   "source": [
    "two_states = False\n",
    "one_mol = False\n",
    "MAX_GEOM = 100"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 2,
   "metadata": {},
   "outputs": [],
   "source": [
    "import sys\n",
    "\n",
    "\n",
    "sys.path.append(\"/home/saxelrod/Repo\")\n",
    "sys.path.append(\"/home/saxelrod/Repo/projects/multi_task\")\n",
    "sys.path.append(\"/home/saxelrod/Repo/projects/multi_task/NeuralForceField\")\n",
    "\n",
    "\n",
    "import os\n",
    "import shutil\n",
    "import numpy as np\n",
    "import matplotlib.pyplot as plt\n",
    "\n",
    "import torch\n",
    "from torch.optim import Adam\n",
    "\n",
    "from nff.data import Dataset, split_train_validation_test\n",
    "import pickle\n",
    "from nff.data.loader import collate_dicts\n",
    "from torch.utils.data import DataLoader\n",
    "\n",
    "\n",
    "from nff.train import Trainer, get_trainer, get_model, loss, hooks, metrics, evaluate"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "It might also be useful setting the GPU you want to use:"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 3,
   "metadata": {},
   "outputs": [],
   "source": [
    "DEVICE = 2\n",
    "OUTDIR = './sandbox'\n",
    "\n",
    "if os.path.exists(OUTDIR):\n",
    "    newpath = os.path.join(os.path.dirname(OUTDIR), 'backup')\n",
    "    if os.path.exists(newpath):\n",
    "        shutil.rmtree(newpath)\n",
    "        \n",
    "    shutil.move(OUTDIR, newpath)"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "## Loading the relevant data"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "As we usually work with the database, we can pack their information in a class `Dataset`, which is a subclass of `torch.utils.data.Dataset`. It basically wraps information on the atomic numbers, energies, forces and SMILES strings for each one of the geometries. In this example, we already have a pre-compiled `Dataset` to be used. We start by loading this file and creating three slices of the original dataset"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 4,
   "metadata": {},
   "outputs": [],
   "source": [
    "import pdb\n",
    "\n",
    "import copy\n",
    "\n",
    "\n",
    "METHOD_NAME = \"sf_tddft_bhhlyp\"\n",
    "FILE_PATH = \"\"\n",
    "DATA_FILE = os.path.join(FILE_PATH, \"data\", \"{}.pickle\".format(METHOD_NAME))\n",
    "with open(DATA_FILE, \"rb\") as f:\n",
    "    props = pickle.load(f)\n",
    "\n",
    "if one_mol:\n",
    "    smiles = props[\"smiles\"][0]\n",
    "    val_idx = [x==smiles for x in props[\"smiles\"]]\n",
    "    for key in props.keys():\n",
    "        props[key] = np.array(props[key])[val_idx].tolist()\n",
    "\n",
    "\n",
    "if two_states:\n",
    "\n",
    "\n",
    "\n",
    "    props[\"energy_0_grad\"] = copy.deepcopy(props[\"force_0\"])\n",
    "    props[\"energy_1_grad\"] = copy.deepcopy(props[\"force_1\"])\n",
    "\n",
    "\n",
    "    for i, element in enumerate(copy.deepcopy(props[\"force_0\"])):\n",
    "        props[\"energy_0_grad\"][i] = -element\n",
    "    for i, element in enumerate(copy.deepcopy(props[\"force_1\"])):\n",
    "        props[\"energy_1_grad\"][i] = -element\n",
    "\n",
    "    props.pop(\"force_0\")\n",
    "    props.pop(\"force_1\")\n",
    "\n",
    "\n",
    "    for key in [\"energy_0\", \"energy_1\", \"energy_0_grad\", \"energy_1_grad\",\n",
    "               \"nxyz\", \"smiles\"]:\n",
    "        if MAX_GEOM is not None:\n",
    "            props[key] = props[key][:MAX_GEOM]\n",
    "\n",
    "    lst = [*props[\"energy_0\"], *props[\"energy_1\"]]\n",
    "    mean = np.mean(lst)\n",
    "\n",
    "    for key in [\"energy_0\", \"energy_1\"]:\n",
    "        lst = copy.deepcopy(props[key])\n",
    "        for i, element in enumerate(lst):\n",
    "            props[key][i] -= mean\n",
    "\n",
    "\n",
    "else:\n",
    "\n",
    "\n",
    "    props.pop(\"force_1\")\n",
    "    props.pop(\"energy_1\")\n",
    "\n",
    "    props[\"energy_0_grad\"] = copy.deepcopy(props[\"force_0\"])\n",
    "    props.pop(\"force_0\")\n",
    "\n",
    "    for i, element in enumerate(props[\"energy_0_grad\"]):\n",
    "        if type(element) is np.ndarray:\n",
    "            props[\"energy_0_grad\"][i] = -element\n",
    "\n",
    "    lst = [en for en in props[\"energy_0\"] if en is not None]\n",
    "    mean = np.mean(lst)\n",
    "\n",
    "    for key in [\"energy_0\"]:\n",
    "        lst = copy.deepcopy(props[key])\n",
    "        for i, element in enumerate(lst):\n",
    "            if element is not None:\n",
    "                lst[i] -= mean\n",
    "\n",
    "        props[key] = lst\n",
    "\n",
    "\n",
    "dataset = Dataset(props=props.copy(), units='atomic')\n",
    "dataset.generate_neighbor_list(cutoff=5)\n",
    "\n",
    "train, val, test = split_train_validation_test(dataset, val_size=0.2, test_size=0.2)"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "The `nff` code interfaces with the `graphbuilder` module through a git submodule in the repository. `graphbuilder` provides methods to create batches of graphs. In `nff`, we interface that through a custom dataloader called `\n",
    "GraphLoader`. Here, we create one loader for each one of the slices."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 5,
   "metadata": {},
   "outputs": [],
   "source": [
    "train_loader = DataLoader(train, batch_size=50, collate_fn=collate_dicts)\n",
    "val_loader = DataLoader(val, batch_size=50, collate_fn=collate_dicts)\n",
    "test_loader = DataLoader(test, batch_size=50, collate_fn=collate_dicts)"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "## Creating a model"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "`nff` is based on SchNet. It parameterizes interatomic interactions in molecules and materials through a series of convolution layers with continuous filters. Here, we are going to create a simple model using the hyperparameters given on `params`:"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 6,
   "metadata": {},
   "outputs": [],
   "source": [
    "n_atom_basis = 256\n",
    "EPS = 1e-15\n",
    " \n",
    "if two_states:\n",
    "    \n",
    "    readoutdict = {\n",
    "                        \"energy_0\": [{'name': 'linear', 'param' : { 'in_features': n_atom_basis, \n",
    "                                                                  'out_features': int(n_atom_basis / 2)}},\n",
    "                                   {'name': 'shifted_softplus', 'param': {}},\n",
    "                                   {'name': 'linear', 'param' : { 'in_features': int(n_atom_basis / 2), \n",
    "                                                                  'out_features': 1}}],\n",
    "                        \"energy_1\": [{'name': 'linear', 'param' : { 'in_features': n_atom_basis, \n",
    "                                                                  'out_features': int(n_atom_basis / 2)}},\n",
    "                                   {'name': 'shifted_softplus', 'param': {}},\n",
    "                                   {'name': 'linear', 'param' : { 'in_features': int(n_atom_basis / 2), \n",
    "                                                                  'out_features': 1}}]\n",
    "                    }\n",
    "    \n",
    "\n",
    "\n",
    "    def post_readout(predict_dict, readoutdict):\n",
    "        sorted_keys = sorted(list(readoutdict.keys()))\n",
    "        sorted_ens = torch.sort(torch.stack([predict_dict[key] for key in sorted_keys]))[0] \n",
    "        sorted_dic = {key: val for key, val in zip(sorted_keys, sorted_ens)}\n",
    "        return sorted_dic\n",
    "\n",
    "\n",
    "        \n",
    "else:\n",
    "    readoutdict = {\n",
    "                        \"energy_0\": [{'name': 'linear', 'param' : { 'in_features': n_atom_basis, \n",
    "                                                                  'out_features': int(n_atom_basis / 2)}},\n",
    "                                   {'name': 'shifted_softplus', 'param': {}},\n",
    "                                   {'name': 'linear', 'param' : { 'in_features': int(n_atom_basis / 2), \n",
    "                                                                  'out_features': 1}}]\n",
    "                    }\n",
    "    \n",
    "    post_readout = None\n",
    "    \n",
    "params = {\n",
    "    'n_atom_basis': n_atom_basis,\n",
    "    'n_filters': 256,\n",
    "    'n_gaussians': 32,\n",
    "    'n_convolutions': 10,\n",
    "    'cutoff': 5.0,\n",
    "    'trainable_gauss': False, \n",
    "    'readoutdict': readoutdict,\n",
    "    'post_readout': post_readout\n",
    "}\n",
    "\n",
    "\n",
    "\n",
    "model = get_model(params)"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "## Creating a trainer"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "To train our model with the data provided, we have to create a loss function. The easiest way to do that is through the `build_mse_loss` builder. Its argument `rho` is a parameter that will multiply the mean square error (MSE) of the force components before summing it with the MSE of the energy."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 7,
   "metadata": {},
   "outputs": [],
   "source": [
    "import numpy as np\n",
    "rho = 0.1\n",
    "decay = 10\n",
    "\n",
    "if two_states:\n",
    "    loss_coef = {'energy_0': rho, 'energy_0_grad': 1, 'energy_1': rho, 'energy_1_grad': 1}\n",
    "else:\n",
    "    loss_coef = {'energy_0': rho, 'energy_0_grad': 1}\n",
    "\n",
    "\n",
    "\n",
    "loss_fn = loss.build_mse_loss(loss_coef=loss_coef)"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "We should also select an optimizer for our recently created model:"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 8,
   "metadata": {},
   "outputs": [],
   "source": [
    "trainable_params = filter(lambda p: p.requires_grad, model.parameters())\n",
    "optimizer = Adam(trainable_params, lr=2e-4)"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "### Metrics and hooks"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "Metrics and hooks allow the customization of the training process. Instead of tweaking directly the code or having to resort to countless flags, we can create submodules (or add-ons) to monitor the progress of the training or customize it."
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "If we want to monitor the progress of our training, say by looking at the mean absolute error (MAE) of energies and forces, we can simply create metrics to observe them:"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 9,
   "metadata": {},
   "outputs": [],
   "source": [
    "if two_states:\n",
    "    train_metrics = [\n",
    "        metrics.MeanAbsoluteError('energy_0'),\n",
    "        metrics.MeanAbsoluteError('energy_0_grad'),\n",
    "        metrics.MeanAbsoluteError('energy_1'),\n",
    "        metrics.MeanAbsoluteError('energy_1_grad'),\n",
    "\n",
    "    ]\n",
    "else:\n",
    "    train_metrics = [\n",
    "        metrics.MeanAbsoluteError('energy_0'),\n",
    "        metrics.MeanAbsoluteError('energy_0_grad')\n",
    "    ]\n",
    "    "
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "Furthermore, if we want to customize how our training procedure is done, we can use hooks which can interrupt or change the train automatically.\n",
    "\n",
    "In our case, we are adding hooks to:\n",
    "* Stop the training procedure after 100 epochs;\n",
    "* Log the training on a machine-readable CSV file under the directory `./sandbox`;\n",
    "* Print the progress on the screen with custom formatting; and\n",
    "* Setup a scheduler for the learning rate."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 10,
   "metadata": {},
   "outputs": [],
   "source": [
    "train_hooks = [\n",
    "    hooks.MaxEpochHook(5000),\n",
    "    hooks.CSVHook(\n",
    "        OUTDIR,\n",
    "        metrics=train_metrics,\n",
    "    ),\n",
    "    hooks.PrintingHook(\n",
    "        OUTDIR,\n",
    "        metrics=train_metrics,\n",
    "        separator = ' | '\n",
    "    ),\n",
    "    hooks.ReduceLROnPlateauHook(\n",
    "        optimizer=optimizer,\n",
    "        patience=30,\n",
    "        factor=0.5,\n",
    "        min_lr=1e-7,\n",
    "        window_length=1,\n",
    "        stop_after_min=True\n",
    "    )\n",
    "]"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "### Trainer wrapper"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "A `Trainer` in the `nff` package is a wrapper to train a model. It automatically creates checkpoints, as well as trains and validates a given model. It also allow further training by loading checkpoints from existing paths, making the training procedure more flexible. Its functionalities can be extended by the hooks we created above. To create a trainer, we have to execute the following command:"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 11,
   "metadata": {},
   "outputs": [],
   "source": [
    "import pdb\n",
    "try:\n",
    "    T = Trainer(\n",
    "        model_path=OUTDIR,\n",
    "        model=model,\n",
    "        loss_fn=loss_fn,\n",
    "        optimizer=optimizer,\n",
    "        train_loader=train_loader,\n",
    "        validation_loader=val_loader,\n",
    "        checkpoint_interval=1,\n",
    "        hooks=train_hooks\n",
    "    )\n",
    "except:\n",
    "    pdb.post_mortem()"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "Now we can finally train the model using the method `train` from the `Trainer`:"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 12,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "               Time | Epoch | Learning rate | Train loss | Validation loss | MAE_energy_0 | MAE_energy_0_grad | GPU Memory (MB)\n",
      "2019-09-16 13:46:00 |     1 |     2.000e-04 | 2459295338.6418 | 1277151616.0000 |   87280.2400 |           34.9220 |            2119\n",
      "2019-09-16 13:46:01 |     2 |     2.000e-04 | 2458654592.2236 | 1276801024.0000 |   87274.6800 |           35.0514 |            2119\n",
      "2019-09-16 13:46:02 |     3 |     2.000e-04 | 2456638388.1095 | 1275590784.0000 |   87247.9400 |           44.7343 |            2119\n",
      "2019-09-16 13:46:03 |     4 |     2.000e-04 | 2449349770.5859 | 1271791872.0000 |   87177.1400 |          114.1421 |            2119\n",
      "2019-09-16 13:46:04 |     5 |     2.000e-04 | 2425140239.9161 | 1263825792.0000 |   87865.5600 |          409.1880 |            2119\n",
      "2019-09-16 13:46:05 |     6 |     2.000e-04 | 2375795221.1718 | 1300772224.0000 |   90725.3000 |         1449.3995 |            2119\n",
      "2019-09-16 13:46:06 |     7 |     2.000e-04 | 2475810267.0612 | 1296837248.0000 |   90526.4600 |         1397.8490 |            2119\n",
      "2019-09-16 13:46:07 |     8 |     2.000e-04 | 2391605412.3797 | 1262215808.0000 |   88288.8800 |          575.4771 |            2119\n",
      "2019-09-16 13:46:08 |     9 |     2.000e-04 | 2399069491.5877 | 1263324672.0000 |   87825.4600 |          380.0161 |            2119\n",
      "2019-09-16 13:46:09 |    10 |     2.000e-04 | 2408758648.9552 | 1263859328.0000 |   87750.7800 |          347.8510 |            2119\n"
     ]
    }
   ],
   "source": [
    "import pdb\n",
    "\n",
    "try:\n",
    "    T.train(device=DEVICE, n_epochs=10)\n",
    "\n",
    "except Exception as e:\n",
    "    print(e)\n",
    "    pdb.post_mortem()\n",
    "    \n"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "## Evaluating the model on the test set"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "Now we have a brand new model trained and validated. We can use the best model from this training to evaluate its performance on the test set. `results` contains the predictions of properties for the whole test dataset. `targets` contains the ground truth for such data. `test_loss` is the loss, calculated with the same function used during the training part"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 13,
   "metadata": {},
   "outputs": [
    {
     "ename": "RuntimeError",
     "evalue": "expected device cpu and dtype Float but got device cuda:2 and dtype Float",
     "output_type": "error",
     "traceback": [
      "\u001b[0;31m---------------------------------------------------------------------------\u001b[0m",
      "\u001b[0;31mRuntimeError\u001b[0m                              Traceback (most recent call last)",
      "\u001b[0;32m<ipython-input-13-4eff7655aa0b>\u001b[0m in \u001b[0;36m<module>\u001b[0;34m()\u001b[0m\n\u001b[0;32m----> 1\u001b[0;31m \u001b[0mresults\u001b[0m\u001b[0;34m,\u001b[0m \u001b[0mtargets\u001b[0m\u001b[0;34m,\u001b[0m \u001b[0mval_loss\u001b[0m\u001b[0;34m,\u001b[0m \u001b[0mother_results\u001b[0m \u001b[0;34m=\u001b[0m \u001b[0mevaluate\u001b[0m\u001b[0;34m(\u001b[0m\u001b[0mmodel\u001b[0m\u001b[0;34m,\u001b[0m \u001b[0mtest_loader\u001b[0m\u001b[0;34m,\u001b[0m \u001b[0mloss_fn\u001b[0m\u001b[0;34m,\u001b[0m \u001b[0mdevice\u001b[0m\u001b[0;34m=\u001b[0m\u001b[0mDEVICE\u001b[0m\u001b[0;34m)\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[0m",
      "\u001b[0;32m~/Repo/projects/multi_task/NeuralForceField/nff/train/evaluate.py\u001b[0m in \u001b[0;36mevaluate\u001b[0;34m(model, loader, loss_fn, device, loss_is_normalized)\u001b[0m\n\u001b[1;32m     27\u001b[0m         \u001b[0mresults\u001b[0m\u001b[0;34m[\u001b[0m\u001b[0;34m\"nxyz\"\u001b[0m\u001b[0;34m]\u001b[0m \u001b[0;34m=\u001b[0m \u001b[0mbatch\u001b[0m\u001b[0;34m[\u001b[0m\u001b[0;34m\"nxyz\"\u001b[0m\u001b[0;34m]\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[1;32m     28\u001b[0m \u001b[0;34m\u001b[0m\u001b[0m\n\u001b[0;32m---> 29\u001b[0;31m         \u001b[0meval_batch_loss\u001b[0m \u001b[0;34m=\u001b[0m \u001b[0mloss_fn\u001b[0m\u001b[0;34m(\u001b[0m\u001b[0mbatch\u001b[0m\u001b[0;34m,\u001b[0m \u001b[0mresults\u001b[0m\u001b[0;34m)\u001b[0m\u001b[0;34m.\u001b[0m\u001b[0mdata\u001b[0m\u001b[0;34m.\u001b[0m\u001b[0mcpu\u001b[0m\u001b[0;34m(\u001b[0m\u001b[0;34m)\u001b[0m\u001b[0;34m.\u001b[0m\u001b[0mnumpy\u001b[0m\u001b[0;34m(\u001b[0m\u001b[0;34m)\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[0m\u001b[1;32m     30\u001b[0m \u001b[0;34m\u001b[0m\u001b[0m\n\u001b[1;32m     31\u001b[0m         \u001b[0;32mif\u001b[0m \u001b[0mloss_is_normalized\u001b[0m\u001b[0;34m:\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n",
      "\u001b[0;32m~/Repo/projects/multi_task/NeuralForceField/nff/train/loss.py\u001b[0m in \u001b[0;36mloss_fn\u001b[0;34m(ground_truth, results)\u001b[0m\n\u001b[1;32m     44\u001b[0m \u001b[0;34m\u001b[0m\u001b[0m\n\u001b[1;32m     45\u001b[0m             \u001b[0;32mif\u001b[0m \u001b[0mlen\u001b[0m\u001b[0;34m(\u001b[0m\u001b[0mtarg\u001b[0m\u001b[0;34m)\u001b[0m \u001b[0;34m!=\u001b[0m \u001b[0;36m0\u001b[0m\u001b[0;34m:\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[0;32m---> 46\u001b[0;31m                 \u001b[0mdiff\u001b[0m \u001b[0;34m=\u001b[0m \u001b[0;34m(\u001b[0m\u001b[0mtarg\u001b[0m \u001b[0;34m-\u001b[0m \u001b[0mpred\u001b[0m \u001b[0;34m)\u001b[0m \u001b[0;34m**\u001b[0m \u001b[0;36m2\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[0m\u001b[1;32m     47\u001b[0m                 \u001b[0merr_sq\u001b[0m \u001b[0;34m=\u001b[0m \u001b[0mcoef\u001b[0m \u001b[0;34m*\u001b[0m \u001b[0mtorch\u001b[0m\u001b[0;34m.\u001b[0m\u001b[0mmean\u001b[0m\u001b[0;34m(\u001b[0m\u001b[0mdiff\u001b[0m\u001b[0;34m)\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[1;32m     48\u001b[0m                 \u001b[0mloss\u001b[0m \u001b[0;34m+=\u001b[0m \u001b[0merr_sq\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n",
      "\u001b[0;31mRuntimeError\u001b[0m: expected device cpu and dtype Float but got device cuda:2 and dtype Float"
     ]
    }
   ],
   "source": [
    "results, targets, val_loss, other_results = evaluate(model, test_loader, loss_fn, device=DEVICE)"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "Finally, we can plot our results to observe how well is our model performing:"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "\n",
    "units = {\n",
    "    'energy_0_grad': r'kcal/mol/$\\AA$',\n",
    "    'energy_0': 'kcal/mol',\n",
    "    'energy_1_grad': r'kcal/mol/$\\AA$',\n",
    "    'energy_1': 'kcal/mol'\n",
    "}\n",
    "\n",
    "dic_keys = list(loss_coef.keys())\n",
    "\n",
    "for i in range(int(len(dic_keys)/2) ):\n",
    "    \n",
    "    fig, ax_fig = plt.subplots(1, 2, figsize=(12, 6))\n",
    "\n",
    "    for ax, key in zip(ax_fig, dic_keys[2*i:2*i+2] ):\n",
    "\n",
    "        pred = torch.cat(results[key]).reshape(-1).detach().numpy()\n",
    "        targ = torch.cat(targets[key]).reshape(-1).detach().numpy()\n",
    "\n",
    "        ax.scatter(pred, targ, color='#ff7f0e', alpha=0.3)\n",
    "\n",
    "        lim_min = min(np.min(pred), np.min(targ)) * 1.1\n",
    "        lim_max = max(np.max(pred), np.max(targ)) * 1.1\n",
    "\n",
    "        ax.set_xlim(lim_min, lim_max)\n",
    "        ax.set_ylim(lim_min, lim_max)\n",
    "\n",
    "        ax.set_aspect('equal')\n",
    "\n",
    "        ax.plot((lim_min, lim_max),\n",
    "                (lim_min, lim_max),\n",
    "                color='#000000',\n",
    "                zorder=-1,\n",
    "                linewidth=0.5)\n",
    "\n",
    "        ax.set_title(key.upper(), fontsize=14)\n",
    "        ax.set_xlabel('predicted %s (%s)' % (key, units[key]), fontsize=12)\n",
    "        ax.set_ylabel('target %s (%s)' % (key, units[key]), fontsize=12)\n",
    "\n",
    "    plt.show()"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "The model is performing quite well."
   ]
  }
 ],
 "metadata": {
  "kernelspec": {
   "display_name": "Python [conda env:anaconda3-neuralff]",
   "language": "python",
   "name": "conda-env-anaconda3-neuralff-py"
  },
  "language_info": {
   "codemirror_mode": {
    "name": "ipython",
    "version": 3
   },
   "file_extension": ".py",
   "mimetype": "text/x-python",
   "name": "python",
   "nbconvert_exporter": "python",
   "pygments_lexer": "ipython3",
   "version": "3.5.6"
  }
 },
 "nbformat": 4,
 "nbformat_minor": 2
}
