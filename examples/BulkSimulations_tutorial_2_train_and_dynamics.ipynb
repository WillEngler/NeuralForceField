{
 "cells": [
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "import sys\n",
    "import numpy as np\n",
    "from importlib import reload\n",
    "import networkx as nx\n",
    "\n",
    "sys.path.insert(0, '/home/wwj/Repo/playgrounds/NeuralForceField/')\n",
    "\n",
    "import torch\n",
    "import torch.nn as nn\n",
    "import copy\n",
    "import torch.nn.functional as F\n",
    "\n",
    "from nff.nn.layers import Dense, GaussianSmearing\n",
    "from nff.nn.modules import GraphDis, SchNetConv, BondEnergyModule, SchNetEdgeUpdate, NodeMultiTaskReadOut\n",
    "from nff.nn.activations import shifted_softplus\n",
    "from nff.nn.graphop import batch_and_sum, get_atoms_inside_cell\n",
    "from nff.nn.utils import get_default_readout\n",
    "\n",
    "from torch.utils.data import DataLoader\n",
    "import nff.data as d\n",
    "import pickle\n",
    "\n",
    "from nff.data import Dataset, split_train_validation_test, collate_dicts\n",
    "\n",
    "from nff.io.ase import * \n",
    "\n",
    "from shutil import rmtreeb"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "# initialize model parameters \n",
    "\n",
    "modelparams = dict()\n",
    "modelparams['n_atom_basis'] = 128\n",
    "modelparams['n_filters'] = 128\n",
    "modelparams['n_gaussians'] = 32\n",
    "modelparams['mol_n_convolutions'] = 3\n",
    "modelparams['sys_n_convolutions'] = 3\n",
    "modelparams['mol_cutoff'] = 4.0\n",
    "modelparams['sys_cutoff'] = 5.0\n",
    "# modelparams[\"V_ex_power\"] = 12\n",
    "# modelparams[\"V_ex_sigma\"] = 4.0"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "from torch.optim import Adam\n",
    "from nff.data import Dataset, split_train_validation_test, collate_dicts, sparsify_tensor\n",
    "from nff.train import Trainer, get_trainer, get_model, loss, hooks, metrics, evaluate\n",
    "from nff.nn.models.hybridgraph import HyBridGraphConvb"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "props = pickle.load( open( \"./data/ethane_data.pkl\", \"rb\" ) )\n",
    "props['offsets'] = [sparsify_tensor(offset.matmul(torch.Tensor(props[\"cell\"][i]))) for i, offset in enumerate(props['offsets'])]\n",
    "dataset = d.Dataset(props.copy(), units='kcal/mol')\n",
    "\n",
    "train, val, test = split_train_validation_test(dataset, val_size=0.1, test_size=0.1)\n",
    "\n",
    "train_loader = DataLoader(train, batch_size=1, collate_fn=collate_dicts)\n",
    "val_loader = DataLoader(val, batch_size=1, collate_fn=collate_dicts)\n",
    "test_loader = DataLoader(test, batch_size=1, collate_fn=collate_dicts)b"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 1,
   "metadata": {},
   "outputs": [],
   "source": [
    "# Training "
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "model = HyBridGraphConv(modelparams)\n",
    "\n",
    "loss_fn = loss.build_mse_loss(loss_coef={'energy_grad': 1})\n",
    "\n",
    "model = HyBridGraphConv(modelparams)\n",
    "\n",
    "\n",
    "trainable_params = filter(lambda p: p.requires_grad, model.parameters())\n",
    "optimizer = Adam(trainable_params, lr=3e-4)\n",
    "\n",
    "\n",
    "train_metrics = [\n",
    "    metrics.MeanAbsoluteError('energy_grad')\n",
    "]\n",
    "\n",
    "from shutil import rmtree\n",
    "import os\n",
    "OUTDIR = \"./CG_test1\"\n",
    "train_hooks = [\n",
    "    hooks.MaxEpochHook(100),\n",
    "    hooks.CSVHook(\n",
    "        OUTDIR,\n",
    "        metrics=train_metrics,\n",
    "    ),\n",
    "    hooks.PrintingHook(\n",
    "        OUTDIR,\n",
    "        metrics=train_metrics,\n",
    "        separator = ' | ',\n",
    "        time_strf='%M:%S'\n",
    "    ),\n",
    "    hooks.ReduceLROnPlateauHook(\n",
    "        optimizer=optimizer,\n",
    "        patience=30,\n",
    "        factor=0.5,\n",
    "        min_lr=1e-7,\n",
    "        window_length=1,\n",
    "        stop_after_min=True\n",
    "    )\n",
    "]\n",
    "\n",
    "if os.path.exists(OUTDIR):\n",
    "    rmtree(OUTDIR)b"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "T = Trainer(\n",
    "    model_path=OUTDIR,\n",
    "    model=model,\n",
    "    loss_fn=loss_fn,\n",
    "    optimizer=optimizer,\n",
    "    train_loader=train_loader,\n",
    "    validation_loader=val_loader,\n",
    "    checkpoint_interval=1,\n",
    "    hooks=train_hooks\n",
    ")"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "T.train(device=0, n_epochs=15)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "# Dynamics "
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "from ase import Atoms\n",
    "from ase.neighborlist import neighbor_list\n",
    "from nff.data.sparse import sparsify_array\n",
    "\n",
    "DEFAULT_CUTOFF = 5.0\n",
    "\n",
    "system_prop = {key: val[0] for key, val in props.items()}\n",
    "system_prop['atoms_cutoff'] = 4.0\n",
    "system_prop['system_cutoff'] = 5.0b"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "from nff.io.ase import BulkPhaseMaterials\n",
    "\n",
    "bulk = BulkPhaseMaterials(numbers=[1, 1] * 64, \n",
    "                               positions=props['nxyz'][0][:, 1:4],\n",
    "                               cell=props['cell'][0],\n",
    "                               pbc=True,\n",
    "                               props=system_prop)\n",
    "bulk.set_masses([15.035, 15.035] * 64) # mass of cg atoms \n",
    "bulk.update_nbr_list()"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "from nff.io import NeuralFF\n",
    "from nff.md.nve import * \n",
    "calc = NeuralFF(model=model, device=0)\n",
    "bulk.set_calculator(calc)\n",
    "nve = Dynamics(bulk, DEFAULTNVEPARAMS)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "nve.run()\n",
    "\n",
    "# save frames as xyz \n",
    "nve.save_as_xyz()"
   ]
  }
 ],
 "metadata": {
  "kernelspec": {
   "display_name": "Python [conda env:nff]",
   "language": "python",
   "name": "conda-env-nff-py"
  },
  "language_info": {
   "codemirror_mode": {
    "name": "ipython",
    "version": 3
   },
   "file_extension": ".py",
   "mimetype": "text/x-python",
   "name": "python",
   "nbconvert_exporter": "python",
   "pygments_lexer": "ipython3",
   "version": "3.7.3"
  }
 },
 "nbformat": 4,
 "nbformat_minor": 2
}
